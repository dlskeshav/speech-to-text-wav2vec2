{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a923f3",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Wav2Vec2 Evaluation with LibriSpeech\n",
    "This notebook demonstrates evaluating the Wav2Vec2 model using Hugging Face Transformers on the LibriSpeech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate jiwer torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Load dataset in streaming mode\n",
    "dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100\", streaming=True)\n",
    "tiny_dataset = list(iter(dataset.take(10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = [], []\n",
    "\n",
    "for i, sample in enumerate(tiny_dataset):\n",
    "    try:\n",
    "        reference = sample[\"text\"].lower()\n",
    "        references.append(reference)\n",
    "\n",
    "        audio = sample[\"audio\"]\n",
    "        waveform = torch.tensor(audio[\"array\"]).unsqueeze(0).float()\n",
    "        sample_rate = audio[\"sampling_rate\"]\n",
    "\n",
    "        inputs = processor(waveform.squeeze(), sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0].lower()\n",
    "\n",
    "        predictions.append(transcription)\n",
    "\n",
    "        print(f\"Sample {i+1}\")\n",
    "        print(f\"Original Text   : {reference}\")\n",
    "        print(f\"Predicted Text  : {transcription}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {i+1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "accuracy = (1 - overall_wer) * 100\n",
    "\n",
    "print(\"ðŸ“Š Evaluation Summary\")\n",
    "print(f\"Word Error Rate (WER): {overall_wer:.4f}\")\n",
    "print(f\"Approximate Accuracy : {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}