{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ed9bdd-ba3a-4f3f-a5b1-3813be1f3711",
   "metadata": {},
   "source": [
    "# Explanation of Hyperparameters and Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721406a7-36f0-43fb-8256-7bd53fe7313b",
   "metadata": {},
   "source": [
    "## (1) Hyperparameters:\n",
    "\n",
    "| Hyperparameter             | Value                          | Explanation |\n",
    "|-----------------------------|--------------------------------|-------------|\n",
    "| **sampling_rate**           | 16000 Hz (16 kHz)              | Audio is resampled to 16kHz because the model expects 16kHz input. |\n",
    "| **return_tensors**          | \"pt\" (PyTorch tensors)         | Input features are returned as PyTorch tensors for model compatibility. |\n",
    "| **model_name**              | \"facebook/wav2vec2-base-960h\"  | Pre-trained model trained on 960 hours of LibriSpeech data. |\n",
    "| **ignore_mismatched_sizes** | True                           | Allows loading pre-trained weights even if some shapes don't match exactly (safe in our case). |\n",
    "| **batch_decode**            | batch_decode(predicted_ids)    | Converts predicted token IDs back into readable text. |\n",
    "\n",
    "🔵 **Note:**  \n",
    "- We did not fine-tune the model here; we only **used the pre-trained model** for inference (prediction).\n",
    "- So no training hyperparameters like learning rate, epochs, optimizer, etc., are used in this basic version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec22f6-0c4a-4ddb-aa5d-f320356f3aaf",
   "metadata": {},
   "source": [
    "## (2) Model Architecture:\n",
    "\n",
    "Wav2Vec2.0 is a **two-stage model**:\n",
    "\n",
    "---\n",
    "\n",
    "### ➡️ 1. Feature Encoder\n",
    "- **Input:** Raw audio waveform (16kHz).\n",
    "- **Working:**  \n",
    "  - Breaks down raw waveform into small chunks.\n",
    "  - Applies several 1D convolution layers.\n",
    "  - Extracts low-level features from audio (like edges in images).\n",
    "\n",
    "---\n",
    "\n",
    "### ➡️ 2. Transformer Encoder\n",
    "- **Input:** Extracted audio features from the Feature Encoder.\n",
    "- **Working:**  \n",
    "  - Applies a **Transformer model** (like used in BERT/Attention models).\n",
    "  - Captures long-term dependencies (context) between different parts of speech.\n",
    "  - Outputs contextualized representations of the audio.\n",
    "\n",
    "---\n",
    "\n",
    "### ➡️ 3. CTC (Connectionist Temporal Classification) Layer\n",
    "- **Input:** Transformer outputs.\n",
    "- **Working:**  \n",
    "  - Predicts sequences of characters (letters) without needing exact alignment between audio and text.\n",
    "  - Decodes the model's output into readable transcriptions.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Full Architecture Diagram (Simple Version):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5254d8-13e0-412b-b457-ed12dbdd13d4",
   "metadata": {},
   "source": [
    "Raw Audio → Feature Encoder → Transformer → CTC Decoder → Text Output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "936ed27f-7da1-40b0-8153-f52223dfd69b",
   "metadata": {},
   "source": [
    "   +------------------+        +---------------------+        +-------------------+        +------------------+\n",
    "   |   Raw Audio      | -----> |  Feature Encoder    | -----> |    Transformer    | -----> |   CTC Decoder    |\n",
    "   |   (.wav file)    |        |  (Conv Layers)      |        |  (Self-Attention) |        |   (Predictions)  |\n",
    "   +------------------+        +---------------------+        +-------------------+        +------------------+\n",
    "                                        |                                          |\n",
    "                                        +------------------------------------------+\n",
    "                                                      |\n",
    "                                                      v\n",
    "                                             +-------------------+\n",
    "                                             |   Text Output     |\n",
    "                                             |   (Transcription) |\n",
    "                                             +-------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c58e6-e2a2-4bc0-aaa2-0fffa5b1ddfd",
   "metadata": {},
   "source": [
    "## ✅ Final Summary:\n",
    "\n",
    "In this project, we used the **Wav2Vec2.0 pre-trained model** by Facebook AI for speech-to-text conversion.  \n",
    "We recorded/provided an audio sample, passed it through a feature extractor, transformer encoder, and decoded the predictions using CTC, resulting in automatic transcription of spoken language into text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4addf16-e4b4-48f5-bc78-252aba616ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
